{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale,Normalizer,OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import Sequential,load_model\n",
    "from tensorflow.python.keras.saving.saved_model import load as saved_model_load\n",
    "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.optimizers \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('task4.training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.read_csv('task4.training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('task4.test.WITHOUT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_answer = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       F\n",
       "1       E\n",
       "2       G\n",
       "3       H\n",
       "4       C\n",
       "       ..\n",
       "9995    G\n",
       "9996    A\n",
       "9997    I\n",
       "9998    B\n",
       "9999    C\n",
       "Name: class, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_last = pd.get_dummies(train['f24'])\n",
    "test_last = pd.get_dummies(test['f24'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA \n",
    "ICA = FastICA(n_components=1, random_state=12) \n",
    "train_last=ICA.fit_transform(train_last)\n",
    "test_last = ICA.fit_transform(test_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns = 'class')\n",
    "train = train.drop(columns = 'f24')\n",
    "test = test.drop(columns = 'f24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = np.array(train,dtype=np.float32)\n",
    "test = np.array(test,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanlist=[]\n",
    "stdlist =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(train)\n",
    "train = sc.transform(train)\n",
    "test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(np.shape(train)[1]):\n",
    "#     tmp = np.sum(train[:,i])\n",
    "#     mean = tmp/np.shape(train)[0]\n",
    "#     meanlist.append(mean)\n",
    "#     std = np.std(train[:,i])\n",
    "#     stdlist.append(std)\n",
    "#     for j in range(np.shape(train)[0]):\n",
    "#         train[j][i] = (train[j][i]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meanlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(np.shape(test)[1]):\n",
    "#     mean = meanlist[i]\n",
    "#     std  = stdlist[i]\n",
    "#     for j in range(np.shape(test)[0]):\n",
    "#         test[j][i] = (test[j][i]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer = Normalizer().fit(train) \n",
    "# train = normalizer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.append(train,train_last,axis=1)\n",
    "test = np.append(test,test_last,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 25)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.get_dummies(train_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_len = np.shape(train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,val_x,train_y,val_y = train_test_split(train,ans,test_size=0.3,random_state=44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree (0.10100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisiontree_ans =  clf.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09842952237011923"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(decisiontree_ans,val_y,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alph = ['A','B','C','D','E','F','G','H','I','J']\n",
    "# prediction_final=[]\n",
    "# for i in range(np.shape(decisiontree_ans)[0]):\n",
    "#     index = np.argmax(decisiontree_ans[i], axis=0)\n",
    "#     prediction_final.append(alph[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = dict([('id',np.array(range(1,2001))),('Predicted',prediction_final)])\n",
    "# tt = pd.DataFrame(tt)\n",
    "# tt.to_csv('test_decision_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (0.01919)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_x, train_y)\n",
    "knn_ans = knn.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019195528086035685"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(knn_ans,val_y,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM(0.1044)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x2,val_x2,train_y2,val_y2 = train_test_split(train,train2['class'].values,test_size=0.3,random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(train_x2, train_y2 )\n",
    "svm_ans = clf.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10446160425910936"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(svm_ans,val_y2,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weka J48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Weka JAR file /usr/share/java/weka.jar not found. Ensure the file is installed or update your environment's WEKA_JAR_PATH to only include valid locations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-459d336448af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mweka\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifiers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/weka/classifiers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     assert os.path.isfile(_cp), (\"Weka JAR file %s not found. Ensure the \" + \\\n\u001b[1;32m     42\u001b[0m         \u001b[0;34m\"file is installed or update your environment's WEKA_JAR_PATH to \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \"only include valid locations.\") % (_cp,)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# http://weka.sourceforge.net/doc.dev/weka/classifiers/Classifier.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Weka JAR file /usr/share/java/weka.jar not found. Ensure the file is installed or update your environment's WEKA_JAR_PATH to only include valid locations."
     ]
    }
   ],
   "source": [
    "from weka.classifiers import Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 1e-3\n",
    "opti =keras.optimizers.Adam(learning_rate=lr)\n",
    "model3 = Sequential()  \n",
    "model3.add(Dense(train_feature_len*1, activation = 'relu',use_bias=True))\n",
    "#model3.add(BatchNormalization())\n",
    "model3.add(Dense(train_feature_len*1, activation = 'relu',use_bias=True))\n",
    "#model3.add(BatchNormalization())\n",
    "model3.add(Dense(train_feature_len*2, activation = 'relu',use_bias=True))\n",
    "#model3.add(BatchNormalization())\n",
    "model3.add(Dense(train_feature_len*1, activation = 'relu',use_bias=True))\n",
    "#model3.add(BatchNormalization())\n",
    "model3.add(Dense(train_feature_len*1, activation = 'relu',use_bias=True))\n",
    "#model3.add(BatchNormalization())\n",
    "model3.add(Dense(10, activation = 'softmax',use_bias=True))\n",
    "para3 = model3.compile(optimizer = opti, loss = 'categorical_crossentropy', metrics = ['acc',f1_m,precision_m, recall_m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3=load_model('ML_task2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=\"weights_with_normalize.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,\n",
    "mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model3 = load_model('weights.best.hdf5',custom_objects={'f1_m':f1_m,'precision_m':precision_m,'recall_m':recall_m})\n",
    "# model3 = load_model('weights_with_normalize.best.hdf5',custom_objects={'f1_m':f1_m,'precision_m':precision_m,'recall_m':recall_m})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Training ------------')\n",
    "# model3.fit(train_x, train_y, validation_split=0.33, epochs=1500,batch_size=128,callbacks=callbacks_list,verbose=0) #2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ------------\n",
      "Epoch 1/10\n",
      "55/55 [==============================] - 0s 1ms/step - loss: 1.3922 - acc: 0.4916 - f1_m: 0.4229 - precision_m: 0.7573 - recall_m: 0.2946\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.3878 - acc: 0.4891 - f1_m: 0.4280 - precision_m: 0.7644 - recall_m: 0.2980\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 0s 1ms/step - loss: 1.3961 - acc: 0.4916 - f1_m: 0.4232 - precision_m: 0.7549 - recall_m: 0.2951\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 0s 1ms/step - loss: 1.4054 - acc: 0.4886 - f1_m: 0.4216 - precision_m: 0.7550 - recall_m: 0.2932\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 0s 1ms/step - loss: 1.4122 - acc: 0.4871 - f1_m: 0.4191 - precision_m: 0.7458 - recall_m: 0.2924\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 0s 3ms/step - loss: 1.4181 - acc: 0.4811 - f1_m: 0.4135 - precision_m: 0.7342 - recall_m: 0.2886\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4125 - acc: 0.4830 - f1_m: 0.4157 - precision_m: 0.7441 - recall_m: 0.2895\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4186 - acc: 0.4804 - f1_m: 0.4137 - precision_m: 0.7395 - recall_m: 0.2885\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 0s 1ms/step - loss: 1.4245 - acc: 0.4803 - f1_m: 0.4111 - precision_m: 0.7437 - recall_m: 0.2850\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 0s 1ms/step - loss: 1.4676 - acc: 0.4739 - f1_m: 0.4009 - precision_m: 0.7239 - recall_m: 0.2783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f87d54b23d0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training ------------')\n",
    "model3.fit(train_x, train_y, epochs=10,batch_size=128) #2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3.save('ML_task2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3 = load_model('weights.best.hdf5',custom_objects={'f1_m':f1_m,'precision_m':precision_m,'recall_m':recall_m})\n",
    "# model3 = load_model('weights_with_normalize.best.hdf5',custom_objects={'f1_m':f1_m,'precision_m':precision_m,'recall_m':recall_m})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ------------\n",
      "\n",
      "test loss:  6.128544807434082\n",
      "\n",
      "test f1_score:  0.059657808393239975\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting ------------')\n",
    "loss, accuracy, f1_score, precision, recall = model3.evaluate(val_x, val_y, verbose=0)\n",
    "print('\\ntest loss: ', loss)\n",
    "print('\\ntest f1_score: ', f1_score) #with normal best 0.02490120939910412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 26)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-67.57402353528985"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model3.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "alph = ['A','B','C','D','E','F','G','H','I','J']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_final=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(np.shape(prediction)[0]):\n",
    "    index = np.argmax(prediction[i], axis=0)\n",
    "    prediction_final.append(alph[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = dict([('id',np.array(range(1,2001))),('Predicted',prediction_final)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.DataFrame(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
